{
    "0": [
        [
            [
                "CoMPM: Context Modeling with Speaker's Pre-trained Memory Tracking for Emotion Recognition in Conversation"
            ]
        ],
        [
            [
                "Abstract"
            ]
        ],
        [
            [
                "As the use of interactive machines grow, the task of Emotion Recognition in Conversation (ERC) became more important."
            ],
            [
                "If the machine-generated sentences reflect emotion, more human-like sympathetic conversations are possible."
            ],
            [
                "Since emotion recognition in conversation is inaccurate if the previous utterances are not taken into account, many studies reflect the dialogue context to improve the performances."
            ],
            [
                "Many recent approaches show performance improvement by combining knowledge into modules learned from external structured data."
            ],
            [
                "However, structured data is difficult to access in non-English languages, making it difficult to extend to other languages."
            ],
            [
                "Therefore, we extract the pre-trained memory using the pre-trained language model as an extractor of external knowledge."
            ],
            [
                "We introduce CoMPM, which combines the speaker's pre-trained memory with the context model, and find that the pre-trained memory significantly improves the performance of the context model."
            ],
            [
                "CoMPM achieves the first or second performance on all data and is state-of-the-art among systems that do not leverage structured data."
            ],
            [
                "In addition, our method shows that it can be extended to other languages because structured knowledge is not required, unlike previous methods."
            ],
            [
                "<edit>",
                3,
                "<add>",
                "Our code is available on github 1 .",
                "</add>",
                "</edit>"
            ],
            [
                "<edit>",
                3,
                "<add>",
                "1 https : //github.com/rungjoo/CoMPM",
                "</add>",
                "</edit>"
            ]
            
        ],
        [
            [
                "Introduction"
            ]
        ],
        [
            [
                "As the number of applications such as interactive chatbots or social media that are used by many users has recently increased dramatically, Emotion Recognition in Conversation (ERC) plays a more important role in natural language processing, and as a proof, a lot of research Ghosal et al., 2020;Jiao et al., 2020) has been conducted on the task."
            ]
        ],
        [
            [
                "The ERC module increases the quality of empathetic conversations with the users and can be utilized when sending tailored push messages to the users (Shin et al., 2019; Zandie and Mahoor, 2020; Lin et al., 2020)."
            ],
            [

                "In addition, emotion recognition can be effectively used for opinion mining, recommender systems, and healthcare systems where it can improve the service qualities by providing personalized results."
            ],
            [
                "As these interactive machines increase, the ERC module plays an increasingly important role."
            ],
            [
                "Figure 1 is an example of a conversation in which two speakers are angry at each other."
            ],
            [
                "The emotion of speaker B's utterance (\"How'd you get to that?\")"
            ],
            [
                "is angry."
            ],
            [
                "If the system does not take into account previous utterances, it is difficult to properly recognize emotions."
            ],
            [
                "Like the previous studies (Ghosal et al., 2020), we show that the utterance-level emotion recognition, which does not consider the previous utterance, have limitations and experiments result in poor performances."
            ]
        ],
        [
            [
                "Therefore, recent studies are attempting to recognize emotions while taking into account the previous utterances."
            ],
            [
                "Representatively, Dia-logueRNN  recognizes the present emotion by tracking context from the previous utterances and the speaker's emotion."
            ],
            [
                "AGHMN (Jiao et al., 2020) considers the previous utterances through memory summarizing using GRU with attention."
            ]
        ],
        [
            [
                "Many recent studies use external knowledge to improve the ERC performance."
            ],
            [
                "<edit>",
                0,
                "However",
                "<punc>",
                ",",
                "</punc>",
                "this",
                "<del>",
                "external",
                "</del>",
                "<add>",
                "exter-nal",
                "</add>",
                "knowledge",
                "is",
                "often",
                "only",
                "available",
                "in",
                "English",
                "<punc>",
                ".",
                "</punc>",
                " </edit>"
            ],
            [
                "In order to utilize the previous methods in languages of other countries, it is expensive and difficult to utilize because external knowledge data must be newly constructed."
            ],
            [
                "In recent NLP studies, due to the effectiveness of the pre-trained language model, it has already been developed in many countries."
            ],
            [
                "Since pre-trained language models are trained by unsupervised learning, these models are relatively usable approaches regardless of language types."
            ],
            [
                "Petroni et al."
            ],
            [
                "(2019) introduces that these language models can be used as knowledge bases and have many advantages over the structured knowledge bases."
            ],
            [
                "Based on these studies, we eliminate the dependence on structured external data used in cutting-edge systems and use a pre-trained language model as a feature extractor of knowledge."
            ]
        ],
        [
            [
                "CoMPM, introduced in this paper, is composed of two modules that take into account previous utterances in dialogue."
            ],
            [
                "(1) The first is a context embedding module (CoM) that reflects all previous utterances as context."
            ],
            [
                "CoM is an auto-regressive model that predicts the current emotion through attention between the previous utterances of the conversation and the current utterance."
            ],
            [
                "(2) The second is a pre-trained memory module (PM) that extracts memory from utterances."
            ],
            [
                "We use the output of the pre-trained language model as the memory embedding where the utterances are passed into the language model."
            ],
            [
                "We use the PM to help predict the emotion of the speaker by taking into account the speaker's linguistic preferences and characteristics."
            ]
        ],
        [
            [
                "We experiment on 4 different English ERC datasets."
            ],
            [
                "Multi-party datasets are MELD  and EmoryNLP (Zahiri and Choi, 2018), and dyadic datasets are IEMOCAP (Busso et al., 2008) and DailyDialog (Li et al., 2017)."
            ],
            [
                "CoMPM achieves the first or second performance according to the evaluation metric compared to all previous systems."
            ],
            [
                "We perform an ablation study on each module to show that the proposed approach is effective."
            ],
            [
                "Further experiments also show that our approach can be used in other languages and show the performance of CoMPM when the number of data is limited."
            ]
        ],
        [
            [
                "Related Work"
            ]
        ],
        [
            [
                "Many recent studies use external knowledge to improve the ERC performance."
            ],
            [
                "<edit>",
                0,
                "KET",
                "(",
                "Zhong",
                "et",
                "al.",
                "<punc>",
                ",",
                "</punc>",
                "2019",
                ")",
                "is",
                "used",
                "as",
                "external",
                "knowledge",
                "based",
                "on",
                "ConceptNet",
                "(",
                "Speer",
                "et",
                "al.",
                "<punc>",
                ",",
                "</punc>",
                "2017",
                ")",
                "and",
                "emotion",
                "<del>",
                "lexicon",
                "</del>",
                "<add>",
                "lex-icon",
                "</add>",
                "NRC_VAD",
                "(",
                "Mohammad",
                "<punc>",
                ",",
                "</punc>",
                "2018",
                ")",
                "as",
                "the",
                "commonsense",
                "knowledge",
                "<punc>",
                ".",
                "</punc>",
                " </edit>"
            ],
            [
                "ConceptNet is a knowledge graph that connects words and phrases in natural language using labeled edges."
            ],
            [
                "NRC_VAD Lexicon has human ratings of valence, arousal, and dominance for more than 20,000 English words."
            ],
            [
                "COSMIC (Ghosal et al., 2020) and Psychological (Li et al., 2021) improve the performance of emotion recognition by extracting commonsense knowledge of the previous utterances."
            ],
            [
                "Commonsense knowledge feature is extracted and leveraged with COMET (Bosselut et al., 2019) trained with ATOMIC (The Atlas of Machine Commonsense) ."
            ],
            [
                "ATOMIC has 9 sentence relation types with inferential if-then commonsense knowledge expressed in text."
            ],
            [
                "ToDKAT (Zhu et al., 2021) improves performance by combining commonsense knowledge using COMET and topic discovery using VHRED (Serban et al., 2017) to the model."
            ]
        ],
        [
            [
                "Ekman (Ekman, 1992) constructs taxonomy of six common emotions (Joy, Sadness, Fear, Anger, Surprise, and Disgust) from human facial expressions."
            ],
            [
                "In addition, Ekman explains that a multimodal view is important for multiple emotions recognition."
            ],
            [
                "The multi-modal data such as MELD and IEMOCAP are some of the available standard datasets for emotion recognition and they are composed of text, speech and vision-based data."
            ],
            [
                "Datcu and Rothkrantz (2014) uses speech and visual information to recognize emotions, and (Alm et al., 2005) attempts to recognize emotions based on text information."
            ],
            [
                "MELD and ICON (Hazarika et al., 2018a) show that the more multi-modal information is used, the better the performance and the text information plays the most important role."
            ],
            [
                "Multimodal information is not always given in most social media, especially in chatbot systems where they are mainly composed of text-based systems."
            ],
            [
                "In this work, we design and introduce a text-based emotion recognition system using neural networks."
            ]
        ],
        [
            [
                "In the previous studies, such as Hazarika et al."
            ],
            [
                "(2018b); Zadeh et al."
            ],
            [
                "(2017); , most works focused on dyadic-party conversation."
            ],
            [
                "However, as the multi-party conversation datasets including MELD and EmoryNLP have become available, a lot of recent research is being conducted on multi-party dialogues such as ; Jiao et al."
            ],
            [
                "(2020); Ghosal et al."
            ],
            [
                "(2020)."
            ],
            [
                "In general, the multi-party conversations have higher speaker dependency than the dyadic-party dialogues, therefore have more conditions to consider and result in poor performance."
            ],
            [
                "Zhou et al."
            ],
            [
                "(2018); Zhang et al."
            ],
            [
                "(2018a) shows that commonsense knowledge is important for understanding conversations and generating appropriate responses."
            ],
            [
                "reports that the lack of external knowledge makes it difficult to classify implicit emotions from the conversation history."
            ],
            [
                "EDA (Bothe et al., 2020) expands the multi-modal emotion datasets by extracting dialog acts from MELD and IEMOCAP and finds out that there is a correlation between dialogue acts and emotion labels."
            ]
        ],
        [
            [
                "Approach"
            ]
        ],
        [
            [
                "Problem Statement"
            ]
        ],
        [
            [
                "In a conversation, M sequential utterances are given as [(u 1 , p u 1 ), (u 2 , p u 2 ), ..., (u M , p u M )]."
            ],
            [
                "u i is the utterance which the speaker p u i uttered, where p u i is one of the conversation participants."
            ],
            [
                "While p u i and p u j (i \u0338 = j) can be the same speaker, the minimum number of the unique conversation participants should be 2 or more."
            ],
            [
                "The ERC is a task of predicting the emotion e t of u t , the utterance of the t-th turn, given the previous utterances h t = {u 1 , ..., u t\u22121 }."
            ],
            [
                "Emotions are labeled as one of the predefined classes depending on the dataset, and the emotions we experimented with are either 6 or 7."
            ],
            [
                "We also experimented with a sentiment classification dataset which provides sentiment labels consisting of positive, negative and neutral."
            ]
        ],
        [
            [
                "Model Overview"
            ]
        ],
        [
            [
                "Figure 2 shows an overview of our model."
            ],
            [
                "Our ERC neural network model is composed of two modules."
            ],
            [
                "The first is CoM which catches the underlying effect of all previous utterances on the current speaker's emotions."
            ],
            [
                "Therefore, we propose a context model to handle the relationship between the current and the previous utterances."
            ],
            [
                "The second one is PM that leverages only the speaker's previous utterances, through which we want to reflect the speaker's knowledge."
            ]
        ],
        [
            [
                "If the CoM and PM are based on different backbones, we consider them to be unaligned with respect to each other's output representations."
            ],
            [
                "Therefore, we design the PM to follow CoM so that the output representations of CoM and PM can mutually understand each other."
            ],
            [
                "If CoM and PM are based on different architectures, CoMPM is trained to understand each other's representations by matching dimensions using W p in Equation 4."
            ],
            [
                "The combination of CoM and PM is described in Section 4.5."
            ]
        ],
        [
            [
                "CoM: Context Embedding Module"
            ]
        ],
        [
            [
                "The context embedding module predicts e t by considering all of the utterances before the t-th turn as the dialogue context."
            ],
            [
                "The example in Figure 2 shows how the model predicts the emotion of u 6 uttered by s A , given a conversation of three participants (s A , s B , s C )."
            ],
            [
                "The previous utterances are"
            ],
            [
                "} and e 6 is predicted while considering the relationship between u 6 and h 6 ."
            ]
        ],
        [
            [
                "We consider multi-party conversations where 2 or more speakers are involved."
            ],
            [
                "A special token <s P > is introduced to distinguish participants in the conversation and to handle the speaker's dependency where P is the set of participants."
            ],
            [
                "In other words, the same special token appears before the utterances of the same speaker."
            ]
        ],
        [
            [
                "We use an Transformer encoder as a context model."
            ],
            [
                "<edit>",
                1,
                "In",
                "many",
                "natural",
                "language",
                "processing",
                "tasks",
                "<punc>",
                ",",
                "</punc>",
                "the",
                "effectiveness",
                "of",
                "the",
                "pre-trained",
                "language",
                "model",
                "has",
                "been",
                "proven",
                "<punc>",
                ",",
                "</punc>",
                "and",
                "we",
                "also",
                "set",
                "the",
                "initial",
                "state",
                "of",
                "the",
                "model",
                "to",
                "RoBERTa",
                "<add>",
                "( Liu et al.,2019 )",
                "</add>",
                "<punc>",
                ".",
                "</punc>",
                " </edit>"
            ],
            [
                "RoBERTa is an unsupervised pre-trained model with largescale open-domain corpora of unlabeled text."
            ]
        ],
        [
            [
                "We use the embedding of the special token <cls> to predict emotion."
            ],
            [
                "<edit>",
                1,
                "The",
                "<",
                "cls",
                ">",
                "token",
                "is",
                "concatenated",
                "at",
                "the",
                "<del>",
                "end",
                "</del>",
                "<add>",
                "beginning",
                "</add>",
                "of",
                "the",
                "input",
                "and",
                "the",
                "output",
                "of",
                "the",
                "context",
                "model",
                "is",
                "as",
                "follows",
                ":",
                " </edit>"
            ]
        ],
        [
            [
                "where P :t\u22121 is the set of speakers in the previous turns."
            ],
            [
                "c t \u2208 R 1\u00d7hc and h c is the dimension of CoM."
            ]
        ],
        [
            [
                "PM: Pre-trained Memory Module"
            ]
        ],
        [
            [
                "External knowledge is known to play an important role in understanding conversation."
            ],
            [
                "Pre-trained language models can be trained on numerous corpora and be used as an external knowledge base."
            ],
            [
                "Inspired by previous studies that the speaker's knowledge helps to judge emotions, we extract and track pre-trained memory from the speaker's previous utterances to utilize the emotions of the current utterance u t ."
            ],
            [
                "If the speaker has never appeared before the current turn, the result of the pre-trained memory is considered a zero vector."
            ],
            [
                "Since <cls> is mostly used for the task of classifying sentences, we use the embedding output of the <cls> token as a vector representing the utterance as follows:"
            ],
            [
                "where p u i = p S , S is the speaker of the current utterance."
            ],
            [
                "k i \u2208 R 1\u00d7h k and h k is the dimension of PM."
            ]
        ],
        [
            [
                "CoMPM: Combination of CoM and PM"
            ]
        ],
        [
            [
                "We combine CoM and PM to predict the speaker's emotion."
            ],
            [
                "In many dialogue systems (Zhang et al., 2018b;Ma et al., 2019), it is known that utterances close to the current turn are important for response."
            ],
            [
                "Therefore, we assume that utterances close to the current utterance will be important in emotional recognition."
            ]
        ],
        [
            [
                "Tracking Method"
            ]
        ],
        [
            [
                "We use k i tracking method using GRU."
            ],
            [
                "The tracking method assumes that the importance of all previous speaker utterances to the current emotion is not equal and varies with the distance of the current utterance."
            ],
            [
                "In other words, since the flow of conversation changes as it progresses, the effect on emotion may differ depending on the distance from the current utterance."
            ],
            [
                "We track and capture the sequential position information of k i using a unidirectional GRU:"
            ]
        ],
        [
            [
                "where t is the turn index of the current utterance, n is the number of previous utterances of the speaker, and i s (s = 1, 2, ..., n) is each turn uttered."
            ],
            [
                "kt t \u2208 R 1\u00d7hc is the output of k in and as a result, the knowledge of distant utterance is diluted and the effect on the current utterance is reduced."
            ],
            [
                "GRU is composed of 2-layers, the dimension of the output vector is h c , and the dropout is set to 0.3 during training."
            ],
            [
                "Finally, the output vector o t is obtained by adding kt t and c t in Equation 4."
            ]
        ],
        [
            [
                "where, W p is a matrix that projects the pretrained memory to the dimension of the context output, and is used only when PM and CoM are different pre-trained language models."
            ]
        ],
        [
            [
                "Emotion Prediction"
            ]
        ],
        [
            [
                "Softmax is applied to the vector multiplied by o t and the linear matrix W o \u2208 R he\u00d7hc to obtain the probability distribution of emotion classes, where h e is the number of emotion classes."
            ],
            [
                "e t is the predicted emotion class that corresponds to the index of the largest probability from the emotion class distribution."
            ]
        ],
        [
            [
                "The objective is to minimize the cross entropy loss so that e t is the same as the ground truth emotional label."
            ]
        ],
        [
            [
                "Experiments"
            ]
        ],
        [
            [
                "Dataset"
            ]
        ],
        [
            [
                "We experiment on four benchmark datasets."
            ],
            [
                "MELD  and EmoryNLP (Zahiri and Choi, 2018) are multi-party datasets, while IEMOCAP (Busso et al., 2008) and DailyDialog (Li et al., 2017) are dyadic-party datasets."
            ],
            [
                "The statistics of the dataset are shown in Table 1."
            ]
        ],
        [
            [
                "IEMOCAP is a dataset involving 10 speakers, and each conversation involves 2 speakers and the emotion-inventory is given as \"happy, sad, angry, excited, frustrated and neutral\"."
            ],
            [
                "The train and development dataset is a conversation involving the previous eight speakers, and the train and development are divided into random splits at a ratio of 9:1."
            ],
            [
                "The test dataset is a conversation involving two later speakers."
            ]
        ],
        [
            [
                "DailyDialog is a dataset of daily conversations between two speakers and the emotion-inventory is given as \"anger, disgust, fear, joy, surprise, sadness and neutral\"."
            ],
            [
                "Since more than 82% of the data are tagged as neutral, neutral emotions are excluded when evaluating systems with Micro-F1 as did in the previous studies."
            ]
        ],
        [
            [
                "MELD is a dataset based on Friends TV show and provides two taxonomy: emotion and sentiment."
            ],
            [
                "MELD's emotion-inventory is given as \"anger, disgust, sadness, joy, surprise, fear and neutrality\" following Ekman (Ekman, 1992) and sentiment-inventory is given as \"positive, negative and neutral\"."
            ],
            [
                "EmoryNLP, like MELD, is also a dataset based on Friends TV show, but the emotion-inventory is given as \"joyful, peaceful, powerful, scared, mad, sad and neutral\"."
            ],
            [
                "Sentiment labels are not provided, but sentiment classes can be grouped as follows: positive: {joyful, peaceful, powerful}, negative: {scared, mad, sad}, neutral: {neutral}"
            ]
        ],
        [
            [
                "Training Setup"
            ]
        ],
        [
            [
                "<edit>",
                1,
                "We",
                "use",
                "the",
                "pre-trained",
                "model",
                "from",
                "the",
                "huggingface",
                "library",
                "<del>",
                "1",
                "</del>",
                "<add>",
                "2",
                "</add>",
                "<punc>",
                ".",
                "</punc>",
                " </edit>"
            ],
            [
                "<edit>",
                3,
                "<del>",
                "1",
                "</del>",
                "<add>",
                "2",
                "</add>",
                "https : //github.com/huggingface/transformers",
                "</edit>"

            ],
            [
                "The optimizer is AdamW and the learning rate is 1e-5 as an initial value."
            ],
            [
                "The learning rate scheduler used for training is get_linear_schedule_with_warmup, and the maximum value of 10 is used for the gradient clipping."
            ],
            [
                "We select the model with the best performance on the validation set."
            ],
            [
                "All experiments are conducted on one V100 GPU with 32GB memory."
            ]
        ],
        [
            [
                "Previous Method"
            ]
        ],
        [
            [
                "We show that the proposed approach is effective by comparing it with various baselines and the stateof-the-art methods."
            ]
        ],
        [
            [
                "KET (Zhong et al., 2019) is a Knowledge Enriched Transformer that reflects contextual utterances with a hierarchical self-attention and leverages external commonsense knowledge by using a context-aware affective graph attention mechanism."
            ]
        ],
        [
            [
                "DialogueRNN  uses a GRU network to keep track of the individual party states in the conversation to predict emotions."
            ],
            [
                "This model assumes that there are three factors in emotion prediction: the speaker, the context from the preceding utterances and the emotion of the preceding utterances."
            ],
            [
                "Also, Ghosal et al."
            ],
            [
                "(2020) shows the performance of RoBERTa+DialogueRNN when the vectors of the tokens are extracted with a pretrained RoBERTa."
            ]
        ],
        [
            [
                "RGAT+P (Ishiwatari et al., 2020) (relational graph attention networks) proposes relational position encodings with sequential information reflecting the relational graph structure, which shows that both the speaker dependency and the sequential information can be captured."
            ]
        ],
        [
            [
                "HiTrans (Li et al., 2020) proposes a transformerbased context- and speaker-sensitive model. Hi-Trans utilize BERT as the low-level transformer to generate local utterance representations, and feed them into another high-level transformer."
            ]
        ],
        [
            [
                "COSMIC (Ghosal et al., 2020) incorporates different elements of commonsense such as mental states, events and causal relations, and learns the relations between participants in the conversation."
            ],
            [
                "This model uses pre-trained RoBERTa as a feature extractor and leverages COMET trained with ATOMIC as the commonsense knowledge."
            ],
            [
                "ERMC-DisGCN (Sun et al., 2021) proposes a discourse-aware graph neural network that utilizes self-speaker dependency of interlocutors as a relational convolution and informative cues of dependent utterances as a gated convolution."
            ]
        ],
        [
            [
                "Psychological (Li et al., 2021) uses commonsense knowledge as enrich edges and processes it with graph transformer."
            ],
            [
                "Psychological performs emotion recognition by utilizing intention of utterances from not only past contexts but also future context."
            ]
        ],
        [
            [
                "DialogueCRN (Hu et al., 2021) introduces an intuitive retrieving process, the reasoning module, which understands both situation-level and speakerlevel contexts."
            ]
        ],
        [
            [
                "ToDKAT (Zhu et al., 2021) proposes a language model with topic detection added, and improves performance by combining it with commonsense knowledge."
            ],
            [
                "<edit>",
                3,
                "<add>",
                "The performance of ToDKAT in MELD was re-released on github 3 .",
                "</add>",
                "</edit>"
            ],
            [
                "<edit>",
                3,
                "<add>",
                "3 https://github.com/something678/TodKat",
                "</add>",
                "</edit>"
            ]
        ],
        [
            [
                "Result and Analysis"
            ]
        ],
        [
            [
                "Table 2 shows the performance of the previous methods and our models."
            ],
            [
                "CoM used alone does not leverage PM and predicts emotions by considering only the dialogue context."
            ],
            [
                "<edit>",
                1,
                "PM",
                "used",
                "alone",
                "<add>",
                "is not used as a memory module",
                "</add>",
                "<punc>",
                ",",
                "</punc>",
                "<add>",
                "but",
                "</add>",
                "<del>",
                "does not consider",
                "</del>",
                "the",
                "<del>",
                "context and",
                "</del>",
                "<add>",
                "same backbone is used.",
                "</add>",
                " </edit>"
            ],
            [
                "<edit>",
                1,
                "<add>",
                "PM used alone",
                "</add>",
                "predicts",
                "<del>",
                "emotions",
                "</del>",
                "<add>",
                "emotion",
                "</add>",
                "only",
                "with",
                "the",
                "utterance",
                "of",
                "the",
                "current",
                "turn",
                "<add>",
                "without considering the context",
                "</add>",
                "<punc>",
                ".",
                "</punc>",
                " </edit>"
            ],
            [
                "CoMPM is a model in which both CoM and PM parameters are updated in the initial state of the pre-trained LM."
            ],
            [
                "CoMPM(f) is a model in which PM parameters are frozen in the initial state (pre-trained LM) and is not trained further, and CoMPM(s) is a model in which PM is trained from scratch."
            ],
            [
                "<edit>",
                3,
                "<add>",
                "CoMPM(k) is a model in which PM is trained on ConceptNet.",
                "</add>",
                "</edit>"
            ],
            [
                "<edit>",
                3,
                "<add>",
                "Following previous studies, we use the average vector for each token in PM(k) as the feature of the utterance.",
                "</add>",
                "</edit>"
            ],
            [
                "<edit>",
                3,
                "<add>",
                "We use the pre-trained model provided by the site 4 as PM(k).",
                "</add>",
                "</edit>"
            ]
        ],
        [
            [
                "The effect of PM can be confirmed through the performance comparison between CoM and CoMPM, and the effect of CoM can be confirmed by comparing the results of CoM and PM."
            ],
            [
                "Since PM does not consider context, it showed worse performance than CoM, and the performance gap is larger in the IEMOCAP dataset with a higher average number of conversation turns."
            ],
            [
                "As a result, we show that the combination of CoM and PM is effective in achieving better performance."
            ]
        ],
        [
            [
                "We confirm the effect of PM structure in the model through the performance of CoMPM(s)."
            ],
            [
                "If PM parameters are not frozen and are instead randomly initialized (i.e."
            ],
            [
                "PM(s)), the performance deteriorates."
            ],
            [
                "CoMPM(s) performs worse than CoMPM, and even performs worse than CoM on the other datasets except for MELD."
            ],
            [
                "That is, PM(s) cannot be regarded as a pre-trained memory because the parameters are randomly initialized, and simply increasing the model complexity does not help to improve the performance."
            ],
            [
                "CoMPM(f) shows similar performance to CoMPM and achieves better performance depending on the data."
            ],
            [
                "PM(f) is not fine-tuned on the data, but it extracts general pre-trained memory from a pretrained language model."
            ],
            [
                "The comparison between PM and PM(f) will be further described in Section 4.6."
            ],
            [
                "<edit>",
                1,
                "<del>",
                "W",
                "</del>",
                "<add>",
                "In addition,CoMPM ( k ) shows better performance than CoM,PM,and CoMPM ( s ) except for IEMOCAP.",
                "</add>",
                " </edit>"
            ],
            [
                "<edit>",
                3,
                "<add>",
                "In IEMOCAP, CoMPM(k) has lower performance than CoM.",
                "</add>",
                "</edit>"
            ],
            [
                "<edit>",
                3,
                "<add>",
                "For all datasets, CoMPM(k) performs slightly worse than CoMPM.",
                "</add>",
                "</edit>"
            ],
            [
                "<edit>",
                3,
                "<add>",
                "In other words, ConceptNet improves the performance of CoMPM, but is not as effective as pretrained memory.",
                "</add>",
                "</edit>"
            ],
            [
                "<edit>",
                1,
                "<del>",
                "e",
                "</del>",
                "<add>",
                "As a result,we",
                "</add>",
                "regard",
                "pre-trained",
                "memory",
                "as",
                "compressed",
                "knowledge",
                "<punc>",
                ",",
                "</punc>",
                "which",
                "can",
                "play",
                "a",
                "role",
                "similar",
                "to",
                "external",
                "knowledge",
                "used",
                "in",
                "<del>",
                "cutting-edge",
                "</del>",
                "<add>",
                "cuttingedge",
                "</add>",
                "systems",
                "<punc>",
                ".",
                "</punc>",
                " </edit>"
            ]
        ],
        [
            [
                "The best performance of our approaches is CoMPM or CoMPM(f), both of which combine pre-trained memory."
            ],
            [
                "<edit>",
                0,
                "We",
                "achieve",
                "state-of-the-art",
                "performance",
                "among",
                "all",
                "systems",
                "that",
                "do",
                "not",
                "<del>",
                "lever-age",
                "</del>",
                "<add>",
                "leverage",
                "</add>",
                "structured",
                "external",
                "data",
                "and",
                "achieve",
                "the",
                "first",
                "or",
                "second",
                "performance",
                "even",
                "including",
                "systems",
                "that",
                "leverage",
                "external",
                "data",
                "<punc>",
                ".",
                "</punc>",
                " </edit>"
            ],
            [
                "Therefore, our approach can be extended to other languages without structured external data as well, which is described in Section 4.7."
            ]
        ],
        [
            [
                "Combinations of CoM and PM"
            ]
        ],
        [
            [
                "We experiment with the effect of pre-trained memory of different language models."
            ],
            [
                "To eliminate the influence of the PM structure, we freeze the parameters of PM and use it as a feature extractor."
            ],
            [
                "Table 3 shows the performance of the pretrained memory extracted by the different language models."
            ],
            [
                "If PM and CoM are based on different backbones, the pre-trained memory is projected through W p as the dimension of the context output."
            ],
            [
                "RoBERTa+BERT and RoBERTa+GPT2 (combination of CoM and PM(f)) have lower performance than RoBERTa+RoBERTa, which is inferred because pre-trained memory of RoBERTa contains richer information than BERT and GPT2."
            ],
            [
                "Since there is a lot of training data in the diallydialog and W p is fine-tuned to the data to mutually understand the pre-trained memory and context representation."
            ],
            [
                "Therefore, we infer that performance does not decrease even if the PM changes from the dailydialog."
            ],
            [
                "However, even if other PMs are used, the performance is improved compared to using only CoM, so the pre-trained memory of other language models is also effective for emotion recognition."
            ]
        ],
        [
            [
                "BERT+RoBERTa has a larger performance decrease than RoBERTa+BERT."
            ],
            [
                "In particular, in IEMOCAP data with a long average number of turns in the context, the performance deteriorates significantly."
            ],
            [
                "In addition, the performance of BERT+RoBERTa is lower than CoM (RoBERTa), which supports that the performance of CoM is a more important factor than the use of pre-trained memory."
            ],
            [
                "In other words, we confirm that CoM is more important than PM in our system for performance, and it is effective to focus on context modeling rather than external knowledge in the study of emotion recognition in conversation."
            ]
        ],
        [
            [
                "Training with Less Data"
            ]
        ],
        [
            [
                "CoMPM is an approach that eliminates dependence on external sources and is easily extensible to any language."
            ],
            [
                "However, the insufficient number of emotional data available in other countries (or actual service) remains a problem."
            ],
            [
                "Therefore, we conduct additional experiments according to the use ratio of training data in MELD and EmoryNLP, where there is neither too much nor too little data."
            ],
            [
                " Figure 3 shows the performance of the model according to the ratio of the training data."
            ],
            [
                "In MELD and EmoryNLP, even if only 60% and 80% are used, respectively, the performance decreases by only 3 points."
            ],
            [
                "Table 2 shows that CoMPM(f) achieves better performance than CoMPM in the emotion classification of IMEOCAP and EmoryNLP, which has fewer training data than other settings."
            ],
            [
                "On the other hand, if there is a lot of training data, CoMPM shows better performance."
            ],
            [
                "Figure 3 shows that as the number of data decreases, CoMPM(f) shows better results than CoMPM, which indicates that it is better to freeze the parameters of PM when the number of training data is insufficient."
            ],
            [
                "Therefore, if there is a lot of training data in the real-world application, CoMPM is expected to achieve good performance, otherwise it is CoMPM(f)."
            ]
        ],
        [
            [
                "ERC in other languages"
            ]
        ],
        [
            [
                "Previous studies mostly utilize external knowledge to improve performance, but these approaches require additional publicly available data, which are mainly available for English."
            ],
            [
                "Indeed, structured knowledge and ERC data are lacking in other languages."
            ],
            [
                "Our approach can be extended to other languages without building additional external knowledge and achieves better performance than simply using a pre-trained model."
            ]
        ],
        [
            [
                "Korean Dataset"
            ]
        ],
        [
            [
                "We constructed data composed of two speakers in Korean, and emotion-inventory is given as \"surprise, fear, ambiguous, sad, disgust, joy, bored, embarrassed, neutral\"."
            ],
            [
                "The total number of sessions is 1000, and the average number of utterance turns is 13.4."
            ],
            [
                "We use the data randomly divided into train:dev:test in a ratio of 8:1:1."
            ],
            [
                "This dataset is for actual service and is not released to the public."
            ],
            [
                "Results in the Korean Dataset"
            ],
            [
                "In Korean, our results are shown in Table 4."
            ],
            [
                "The backbone of CoM and PM is Korean-BERT owned by the company, respectively."
            ],
            [
                "In the Korean dataset, like the English dataset, the performance is good in the order of CoMPM, CoM, and PM."
            ],
            [
                "<edit>",
                1,
                "Our",
                "approach",
                "simply",
                "shows",
                "a",
                "significant",
                "performance",
                "improvement",
                "on",
                "baselines",
                "that",
                "are",
                "fine-tuned",
                "to",
                "the",
                "language",
                "model",
                "and",
                "works",
                "well",
                "for",
                "other",
                "languages",
                "as",
                "well",
                "as",
                "for",
                "English",
                "<del>",
                "data",
                "</del>",
                "<punc>",
                ".",
                "</punc>",
                " </edit>"
            ]
        ],
        [
            [
                
                "Conclusion"
            ]
        ],
        [
            [
                "We propose CoMPM that leverages pre-trained memory using a pre-trained language model."
            ],
            [
                "CoMPM consists of a context embedding module (CoM) and a pre-trained memory module (PM), and the experimental results show that each module is effective in improving the performance."
            ],
            [
                "CoMPM outperforms baselines on both dyadic-party and multi-party datasets and achieves state-of-the-art among systems that do not use external knowledge."
            ],
            [
                "In addition, CoMPM achieves performance comparable to cutting-edge systems that leverage structured external knowledge, which is the effect of pre-trained memory of the language model."
            ],
            [
                "By combining other pre-trained memories, we find that the pre-trained memory extracted with RoBERTa is richer and more effective than the pre-trained memory extracted with BERT or GPT2."
            ],
            [
                "Since we believe that pre-trained memory is proportional to the performance of a language model, a language model with a large training corpus and many parameters is considered to be more effective."
            ],
            [
                "However, we find that context modeling is more important than pre-trained memory for emotion recognition in conversation, and future research will focus on context modeling."
            ]
        ],
        [
            [
                "Additionally, our approach achieves competitive performance and does not require externally structured data."
            ],
            [
                "Therefore, we show that it can be easily extended to Korean as well as English, and it is expected to be effective in other countries."
            ]
        ]
    ],
    "1": [
        [
            [
                "DiffCSE: Difference-based Contrastive Learning for Sentence Embeddings"
            ]
        ],
        [
            [
                "Abstract"
            ]
        ],
        [
            [
                "We propose DiffCSE, an unsupervised contrastive learning framework for learning sentence embeddings."
            ],
            [
                "DiffCSE learns sentence embeddings that are sensitive to the difference between the original sentence and an edited sentence, where the edited sentence is obtained by stochastically masking out the original sentence and then sampling from a masked language model."
            ],
            [
                "We show that DiffSCE is an instance of equivariant contrastive learning (Dangovski et al., 2021), which generalizes contrastive learning and learns representations that are insensitive to certain types of augmentations and sensitive to other \"harmful\" types of augmentations."
            ],
            [
                "<edit>",
                1,
                "Our",
                "experiments",
                "show",
                "that",
                "<del>",
                "Dif-fCSE",
                "</del>",
                "<add>",
                "DiffCSE",
                "</add>",
                "achieves",
                "state-of-the-art",
                "results",
                "among",
                "unsupervised",
                "sentence",
                "representation",
                "learning",
                "methods",
                "<punc>",
                ",",
                "</punc>",
                "outperforming",
                "unsupervised",
                "SimCSE",
                "<add>",
                "1",
                "</add>",
                "by",
                "2.3",
                "absolute",
                "points",
                "on",
                "semantic",
                "textual",
                "similarity",
                "tasks",
                "<punc>",
                ".",
                "</punc>",
                "<del>",
                "1",
                "</del>",
                "<add>",
                "2",
                "</add>",
                "</edit>"
            ],
            [
                "<edit>",
                3,
                "1",
                "<add>",
                "SimCSE has two settings : unsupervised and supervised.",
                "</add>",
                "</edit>"
            ],
            [
                "<edit>",
                3,
                "<add>",
                "In this paper, we focus on the unsupervised setting.",
                "</add>",
                "</edit>"
            ],
            [
                "<edit>",
                3,
                "<add>",
                "Unless otherwise stated, in this paper we use SimCSE to refer to unsupervised SimCSE.",
                "</add>",
                "</edit>"
            ],
            [
                "<edit>",
                3,
                "<add>",
                "2",
                "</add>",
                "Pretrained models and code",
                "<del>",
                 "will be made", 
                "</del>",
                "<add>",
                "are",
                "</add>",
                "available",
                "<add>",
                "at https:// github.com/voidism/DiffCSE.",
                "</add>",
                "</edit>"
            ]
           
        ],
        [
            [
                "Introduction"
            ]
        ],
        [
            [
                "Learning \"universal\" sentence representations that capture rich semantic information and are at the same time performant across a wide range of downstream NLP tasks without task-specific finetuning is an important open issue in the field (Conneau et al., 2017;Cer et al., 2018;Kiros et al., 2015;Logeswaran and Lee, 2018;Giorgi et al., 2020;Yan et al., 2021;Gao et al., 2021)."
            ],
            [
                "Recent work has shown that finetuning pretrained language models with contrastive learning makes it possible to learn good sentence embeddings without any labeled data (Giorgi et al., 2020;Yan et al., 2021;Gao et al., 2021)."
            ],
            [
                "Contrastive learning uses multiple augmentations on a single datum to construct positive pairs whose representations are trained to"
            ],
            [
                "be more similar to one another than negative pairs."
            ],
            [
                "While different data augmentations (random cropping, color jitter, rotations, etc.)"
            ],
            [
                "<edit>",
                0,
                "have",
                "been",
                "found",
                "to",
                "be",
                "crucial",
                "for",
                "pretraining",
                "vision",
                "models",
                "(",
                "Chen",
                "et",
                "al.",
                "<punc>",
                ",",
                "</punc>",
                "2020",
                ")",
                "<punc>",
                ",",
                "</punc>",
                "such",
                "augmentations",
                "have",
                "generally",
                "been",
                "<del>",
                "un-successful",
                "</del>",
                "<add>",
                "unsuccessful",
                "</add>",
                "when",
                "applied",
                "to",
                "contrastive",
                "learning",
                "of",
                "sentence",
                "embeddings",
                "<punc>",
                ".",
                "</punc>",
                " </edit>"
            ],
            [
                "Indeed, Gao et al."
            ],
            [
                "(2021) find that constructing positive pairs via a simple dropout-based augmentation works much better than more complex augmentations such as word deletions or replacements based on synonyms or masked language models."
            ],
            [
                "This is perhaps unsurprising in hindsight; while the training objective in contrastive learning encourages representations to be invariant to augmentation transformations, direct augmentations on the input (e.g., deletion, replacement) often change the meaning of the sentence."
            ],
            [
                "That is, ideal sentence embeddings should not be invariant to such transformations."
            ]
        ],
        [
            [
                "We propose to learn sentence representations that are aware of, but not necessarily invariant to, such direct surface-level augmentations."
            ],
            [
                "This is an instance of equivariant contrastive learning (Dangovski et al., 2021), which improves vision representation learning by using a contrastive loss on insensitive image transformations (e.g., grayscale) and a prediction loss on sensitive image transformations (e.g., rotations)."
            ],
            [
                "We operationalize equivariant contrastive learning on sentences by using dropout-based augmentation as the insensitive transformation (as in SimCSE (Gao et al., 2021)) and MLM-based word replacement as the sensitive transformation."
            ],
            [
                "This results in an additional crossentropy loss based on the difference between the original and the transformed sentence."
            ]
        ],
        [
            [
                "We conduct experiments on 7 semantic textual similarity tasks (STS) and 7 transfer tasks from Sen-tEval (Conneau and Kiela, 2018) and find that this difference-based learning greatly improves over standard contrastive learning."
            ],
            [
                "<edit>",
                0,
                "Our",
                "DiffCSE",
                "approach",
                "can",
                "achieve",
                "around",
                "2.3",
                "%",
                "absolute",
                "<del>",
                "improvement",
                "</del>",
                "<add>",
                "improve- ment",
                "</add>",
                "on",
                "STS",
                "datasets",
                "over",
                "SimCSE",
                "<punc>",
                ",",
                "</punc>",
                "the",
                "previous",
                "state-of-the-art",
                "model",
                "<punc>",
                ".",
                "</punc>",
                " </edit>"
            ],
            [
                "We also conduct a set of ablation studies to justify our designed architecture."
            ]
        ],
        [
            [
                "Qualitative study and analysis are also included to look into the embedding space of DiffCSE."
            ]
        ],
        [
            [
                "2 Background and Related Work"
            ]
        ],
        [
            [
                "Learning Sentence Embeddings"
            ]
        ],
        [
            [
                "Learning universal sentence embeddings has been studied extensively in prior work, including unsupervised approaches such as Skip-Thought (Kiros et al., 2015), Quick-Thought (Logeswaran and Lee, 2018) and FastSent (Hill et al., 2016), or supervised methods such as InferSent (Conneau et al., 2017), Universal Sentence Encoder (Cer et al., 2018) and Sentence-BERT (Reimers and Gurevych, 2019)."
            ],
            [
                "Recently, researchers have focused on (unsupervised) contrastive learning approaches such as Sim-CLR (Chen et al., 2020) to learn sentence embeddings."
            ],
            [
                "SimCLR (Chen et al., 2020) learns image representations by creating semantically close augmentations for the same images and then pulling these representations to be closer than representations of random negative examples."
            ],
            [
                "The same framework can be adapted to learning sentence embeddings by designing good augmentation methods for natural language."
            ],
            [
                "ConSERT (Yan et al., 2021) uses a combination of four data augmentation strategies: adversarial attack, token shuffling, cut-off, and dropout."
            ],
            [
                "<edit>",
                0,
                "DeCLUTR",
                "(",
                "Giorgi",
                "et",
                "al.",
                "<punc>",
                ",",
                "</punc>",
                "2020",
                ")",
                "uses",
                "overlapped",
                "spans",
                "as",
                "positive",
                "<del>",
                "examples",
                "</del>",
                "<add>",
                "ex-amples",
                "</add>",
                "and",
                "distant",
                "spans",
                "as",
                "negative",
                "examples",
                "for",
                "learning",
                "contrastive",
                "span",
                "representations",
                "<punc>",
                ".",
                "</punc>",
                " </edit>"
            ],
            [
                "Finally, SimCSE (Gao et al., 2021) proposes an extremely simple augmentation strategy by just switching dropout masks."
            ],
            [
                "While simple, sentence embeddings learned in this manner have been shown to be better than other more complicated augmentation methods."
            ]
        ],
        [
            [
                "Equivariant Contrastive Learning"
            ]
        ],
        [
            [
                "DiffCSE is inspired by a recent generalization of contrastive learning in computer vision (CV) called equivariant contrastive learning (Dangovski et al., 2021)."
            ],
            [
                "We now explain how this CV technique can be adapted to natural language."
            ]
        ],
        [
            [
                "Understanding the role of input transformations is crucial for successful contrastive learning."
            ],
            [
                "Past empirical studies have revealed useful transformations for contrastive learning, such as random resized cropping and color jitter for computer vision (Chen et al., 2020) and dropout for NLP (Gao et al., 2021)."
            ],
            [
                "Contrastive learning encourages representations to be insensitive to these transformations, i.e."
            ],
            [
                "the encoder is trained to be invariant to a set of manually chosen transformations."
            ],
            [
                "The above studies in CV and NLP have also revealed transformations that are harmful for contrastive learning."
            ],
            [
                "For example, Chen et al."
            ],
            [
                "(2020) showed that making the representations insensitive to rotations decreases the ImageNet linear probe accuracy, and Gao et al."
            ],
            [
                "(2021) showed that using an MLM to replace 15% of the words drastically reduces performance on STS-B."
            ],
            [
                "While previous works simply omit these transformations from contrastive pre-training, here we argue that we should still make use of these transformations by learning representations that are sensitive (but not necessarily invariant) to such transformations."
            ]
        ],
        [
            [
                "The notion of (in)sensitivity can be captured by the more general property of equivariance in mathematics."
            ],
            [
                "Let T be a transformation from a group G and let T (x) denote the transformation of a sentence x. Equivariance is the property that there is an induced group transformation T \u2032 on the output features (Dangovski et al., 2021):"
            ]
        ],
        [
            [
                "In the special case of contrastive learning, T \u2032 's target is the identity transformation, and we say that f is trained to be \"invariant to T .\""
            ],
            [
                "However, invariance is just a trivial case of equivariance, and we can design training objectives where T \u2032 is not the identity for some transformations (such as MLM), while it is the identity for others (such as dropout)."
            ],
            [
                "Dangovski et al."
            ],
            [
                "(2021) show that generalizing contrastive learning to equivariance in this way improves the semantic quality of features in CV, and here we show that the complementary nature of invariance and equivariance extends to the NLP domain."
            ],
            [
                "The key observation is that the encoder should be equivariant to MLM-based augmentation instead of being invariant."
            ],
            [
                "We can operationalize this by using a conditional discriminator that combines the sentence representation with an edited sentence, and then predicts the difference between the original and edited sentences."
            ],
            [
                "This is essentially a conditional version of the ELEC-TRA model (Clark et al., 2020), which makes the encoder equivariant to MLM by using a binary discriminator which detects whether a token is from the original sentence or from a generator."
            ],
            [
                "We hypothesize that conditioning the ELECTRA model with the representation from our sentence encoder is a useful objective for encouraging f to be \"equivariant to MLM.\""
            ]
        ],
        [
            [
                "To the best of our knowledge, we are the first to observe and highlight the above parallel between CV and NLP."
            ],
            [
                "In particular, we show that equivariant contrastive learning extends beyond CV, and that it works for transformations even without algebraic structures, such as diff operations on sentences."
            ],
            [
                "<edit>",
                0,
                "Further",
                "<punc>",
                ",",
                "</punc>",
                "insofar",
                "as",
                "the",
                "canonical",
                "set",
                "of",
                "useful",
                "transformations",
                "is",
                "less",
                "established",
                "in",
                "NLP",
                "than",
                "is",
                "in",
                "CV",
                "<punc>",
                ",",
                "</punc>",
                "DiffCSE",
                "can",
                "serve",
                "as",
                "a",
                "diagnostic",
                "tool",
                "for",
                "NLP",
                "researchers",
                "to",
                "discover",
                "useful",
                "<del>",
                "trans-formations",
                "</del>",
                "<add>",
                "transformations",
                "</add>",
                "<punc>",
                ".",
                "</punc>",
                " </edit>"
            ]
        ],
        [
            [
                "Difference-based Contrastive Learning"
            ]
        ],
        [
            [
                "Our approach is straightforward and can be seen as combining the standard contrastive learning objective from SimCSE (Figure 1, left) with a difference prediction objective which conditions on the sentence embedding (Figure 1,right)."
            ]
        ],
        [
            [
                "Given an unlabeled input sentence x, SimCSE creates a positive example x + for it by applying different dropout masks."
            ],
            [
                "By using the BERT base encoder f , we can obtain the sentence embedding h = f (x) for x (see section 4 for how h is obtained)."
            ],
            [
                "The training objective for SimCSE is:"
            ]
        ],
        [
            [
                ", where N is the batch size for the input batch {x i } N i=1 as we are using in-batch negative examples, sim(\u2022, \u2022) is the cosine similarity function, and \u03c4 is a temperature hyperparameter."
            ]
        ],
        [
            [
                "On the right-hand side of Figure 1 is a conditional version of the difference prediction objective used in ELECTRA (Clark et al., 2020), which contains a generator and a discriminator."
            ]
        ],
        [
            [
                "Given a sentence of length T , x = [x (1) , x (2) , ..., x (T ) ], we first apply a random mask m = [m (1) , m (2) , ..., m (T ) ], m (t) \u2208 [0, 1] on x to obtain x \u2032 = m \u2022 x."
            ],
            [
                "We use another pretrained MLM as the generator G to perform masked language modeling to recover randomly masked tokens in x \u2032 to obtain the edited sentence x \u2032\u2032 = G(x \u2032 )."
            ],
            [
                "Then, we use a discriminator D to perform the Replaced Token Detection (RTD) task."
            ],
            [
                "For each token in the sentence, the model needs to predict whether it has been replaced or not."
            ],
            [
                "The cross-entropy loss for a single sentence x is:"
            ]
        ],
        [
            [
                "And the training objective for a batch is"
            ]
        ],
        [
            [
                "Finally we optimize these two losses together with a weighting coefficient \u03bb:"
            ]
        ],
        [
            [
                "The difference between our model and ELECTRA is that our discriminator D is conditional, so it can use the information of x compressed in a fixeddimension vector h = f (x)."
            ],
            [
                "The gradient of D can be backward-propagated into f through h. By doing so, f will be encouraged to make h informative enough to cover the full meaning of x, so that D can distinguish the tiny difference between x and x \u2032\u2032 ."
            ],
            [
                "This approach essentially makes the conditional discriminator perform a \"diff operation\", hence the name DiffCSE."
            ]
        ],
        [
            [
                "When we train our DiffCSE model, we fix the generator G, and only the sentence encoder f and the discriminator D are optimized."
            ],
            [
                "After training, we discard D and only use f (which remains fixed) to extract sentence embeddings to evaluate on the downstream tasks."
            ]
        ],
        [
            [
                "Experiments"
            ]
        ],
        [
            [
                "Setup"
            ]
        ],
        [
            [
                "In our experiment, we follow the setting of unsupervised SimCSE (Gao et al., 2021) and build our model based on their PyTorch implementation."
            ],
            [
                "<edit>",
                1,
                "<del>",
                "2",
                "</del>",
                "<add>",
                "3",
                "</add>",
                "We",
                "also",
                "use",
                "the",
                "checkpoints",
                "of",
                "BERT",
                "(",
                "Devlin",
                "et",
                "al.",
                "<punc>",
                ",",
                "</punc>",
                "2019",
                ")",
                "and",
                "RoBERTa",
                "(",
                "Liu",
                "et",
                "al.",
                "<punc>",
                ",",
                "</punc>",
                "2019",
                ")",
                "as",
                "the",
                "initialization",
                "of",
                "our",
                "sentence",
                "encoder",
                "f",
                "<punc>",
                ".",
                "</punc>",
                " </edit>"
            ],
            [
                "We add an MLP layer with Batch Normalization (Ioffe and Szegedy, 2015) (BatchNorm) on top of the [CLS] representation as the sentence embedding."
            ],
            [
                "We will compare the model with/without BatchNorm in section 5."
            ],
            [
                "For the discriminator D, we use the same model as the sentence encoder f (BERT/RoBERTa)."
            ],
            [
                "For the generator G, we use the smaller DistilBERT and DistilRoBERTa (Sanh et al., 2019) for efficiency."
            ],
            [
                "Note that the generator is fixed during training unlike the ELECTRA paper (Clark et al., 2020)."
            ],
            [
                "We will compare the results of using different size model for the generator in section 5."
            ],
            [
                "More training details are shown in Appendix A."
            ]
        ],
        [
            [
                "Data"
            ]
        ],
        [
            [
                "For unsupervised pretraining, we use the same 10 6 randomly sampled sentences from English Wikipedia that are provided by the source code of SimCSE."
            ],
            [
                "<edit>",
                1,
                "<del>",
                "2",
                "</del>",
                "<add>",
                "3",
                "</add>",
                "We",
                "evaluate",
                "our",
                "model",
                "on",
                "7",
                "semantic",
                "textual",
                "similarity",
                "(",
                "STS",
                ")",
                "and",
                "7",
                "transfer",
                "tasks",
                "in",
                "SentEval",
                "<punc>",
                ".",
                "</punc>",
                " </edit>"
            ],
            [
                "<edit>",
                1,
                "<del>",
                "3",
                "</del>",
                "<add>",
                "4",
                "</add>",
                "STS",
                "tasks",
                "includes",
                "STS",
                "2012-2016",
                "(",
                "Agirre",
                "et",
                "al.",
                "<punc>",
                ",",
                "</punc>",
                "2016",
                ")",
                "<punc>",
                ",",
                "</punc>",
                "STS",
                "Benchmark",
                "(",
                "Cer",
                "et",
                "al.",
                "<punc>",
                ",",
                "</punc>",
                "2017",
                ")",
                "and",
                "SICK-Relatedness",
                "(",
                "Marelli",
                "et",
                "al.",
                "<punc>",
                ",",
                "</punc>",
                "2014",
                ")",
                "<punc>",
                ".",
                "</punc>",
                " </edit>"
            ],
            [
                "All the STS experiments are fully unsupervised, which means no STS training datasets are used and all embeddings are fixed once they are trained."
            ],
            [
                "The transfer tasks are various sentence classification tasks, including MR (Pang and Lee, 2005), CR (Hu and Liu, 2004), SUBJ (Pang and Lee, 2004), MPQA (Wiebe et al., 2005), SST-2 (Socher et al., 2013), TREC (Voorhees and Tice, 2000) and MRPC (Dolan and Brockett, 2005)."
            ],
            [
                "In these transfer tasks, we will use a logistic regression classifier trained on top of the frozen sentence embeddings, following the standard setup (Conneau and Kiela, 2018)."
            ]
        ],
        [
            [
                "Results"
            ]
        ],
        [
            [
                "Baselines We compare our model with many strong unsupervised baselines including Sim-CSE (Gao et al., 2021), IS-BERT (Zhang et al., 2020), CMLM , De-CLUTR (Giorgi et al., 2020), CT-BERT (Carlsson et al., 2021), SG-OPT (Kim et al., 2021) and some post-processing methods like BERT-flow (Li et al., 2020) and BERT-whitening (Su et al., 2021) along with some naive baselines like averaged GloVe embeddings (Pennington et al., 2014) and averaged first and last layer BERT embeddings."
            ]
        ],
        [
            [
                "We show the results of STS tasks in Table 1 including BERT base (upper part) and RoBERTa base (lower part)."
            ],
            [
                "We also reproduce the previous state-ofthe-art SimCSE (Gao et al., 2021 the results of SimCSE with MLM, DiffCSE still can have a little improvement around 0.2%."
            ]
        ],
        [
            [
                "Ablation Studies"
            ]
        ],
        [
            [
                "In the following sections, we perform an extensive series of ablation studies that support our model design."
            ],
            [
                "We use BERT base model to evaluate on the development set of STS-B and transfer tasks."
            ]
        ],
        [
            [
                "Removing Contrastive Loss In our model, both the contrastive loss and the RTD loss are crucial because they maintain what should be sensitive and what should be insensitive respectively."
            ],
            [
                "If we remove the RTD loss, the model becomes a SimCSE model; if we remove the contrastive loss, the performance of STS-B drops significantly by 30%, while the average score of transfer tasks also drops by 2% (see Table 3)."
            ],
            [
                "This result shows that it is important to have insensitive and sensitive attributes that exist together in the representation space."
            ]
        ],
        [
            [
                "Next Sentence vs."
            ],
            [
                "Same Sentence Some methods for unsupervised sentence embeddings like Quick-Thoughts (Logeswaran and Lee, 2018) and CMLM  predict the next sentence as the training objective."
            ],
            [
                "We also experiment with a variant of DiffCSE by conditioning the ELECTRA loss based on the next sentence."
            ]
        ],
        [
            [
                "Note that this kind of model is not doing a \"diff operation\" between two similar sentences, and is not an instance of equivariant contrastive learning."
            ]
        ],
        [
            [
                "As shown in Table 3 (use next sent."
            ],
            [
                "for x \u2032 ), the score of STS-B decreases significantly compared to DiffCSE while transfer performance remains similar."
            ],
            [
                "We also tried using the same sentence and the next sentence at the same time for conditioning the ELECTRA objective (use same+next sent."
            ],
            [
                "for x \u2032 ), and did not observe improvements."
            ]
        ],
        [
            [
                "<edit>",
                1,
                "Other",
                "Conditional",
                "Pretraining",
                "Tasks",
                "Instead",
                "of",
                "a",
                "conditional",
                "binary",
                "difference",
                "prediction",
                "loss",
                "<punc>",
                ",",
                "</punc>",
                "we",
                "can",
                "also",
                "consider",
                "other",
                "conditional",
                "pretraining",
                "tasks",
                "such",
                "as",
                "a",
                "conditional",
                "MLM",
                "objective",
                "proposed",
                "by",
                "<punc>",
                ",",
                "</punc>",
                "or",
                "corrective",
                "language",
                "modeling",
                "<punc>",
                ",",
                "</punc>",
                "<del>",
                "4",
                "</del>",
                "<add>",
                "5",
                "</add>",
                "proposed",
                "by",
                "COCO-LM",
                "(",
                "Meng",
                "et",
                "al.",
                "<punc>",
                ",",
                "</punc>",
                "2021",
                ")",
                "<punc>",
                ".",
                "</punc>",
                " </edit>"
            ],
            [
                "We experiment with these objectives instead of the difference prediction objective in Table 3."
            ]
        ],
        [
            [
                "We observe that conditional MLM on the same sentence does not improve the performance either on STS-B or transfer tasks compared with DiffCSE."
            ],
            [
                "Conditional MLM on the next sentence performs even worse for STS-B, but slightly better than using the same sentence on transfer tasks."
            ],
            [
                "Using both the same and the next sentence also does not improve the performance compared with DiffCSE."
            ],
            [
                "For the corrective LM objective, the performance of STS-B decreases significantly compared with DiffCSE."
            ],
            [
                "<edit>",
                0,
                "<del>",
                "domly",
                "</del>",
                "<add>",
                "randomly",
                "</add>",
                "insert",
                "mask",
                "tokens",
                "to",
                "the",
                "sentence",
                "<punc>",
                ",",
                "</punc>",
                "and",
                "then",
                "use",
                "a",
                "generator",
                "to",
                "convert",
                "mask",
                "tokens",
                "into",
                "real",
                "tokens",
                "<punc>",
                ".",
                "</punc>",
                " </edit>"
            ],
            [
                "The number of inserted masked tokens is 15% of the sentence length."
            ],
            [
                "The task is to predict whether a token is an inserted token or the original token."
            ],
            [
                "For deletion, we randomly delete 15% tokens in the sentence, and the task is to predict for each token whether a token preceding it has been deleted or not."
            ],
            [
                "The results are shown in Table 4."
            ],
            [
                "We can see that using either insertion or deletion achieves a slightly worse STS-B performance than using MLM replacement."
            ],
            [
                "For transfer tasks, their results are similar."
            ],
            [
                "Finally, we find that combining all three augmentations in the training process does not improve the MLM replacement strategy."
            ]
        ],
        [
            [
                "Pooler Choice In SimCSE, the authors use the pooler in BERT's original implementation (one linear layer with tanh activation function) as the final layer to extract features for computing contrastive loss."
            ],
            [
                "In our implementation (see details in Appendix A), we find that it is better to use a two-layer pooler with Batch Normalization (Batch-Norm) (Ioffe and Szegedy, 2015), which is commonly used in contrastive learning framework in computer vision (Chen et al., 2020;Grill et al., 2020;Chen and He, 2021;Hua et al., 2021)."
            ],
            [
                "We show the ablation results in Table 5."
            ],
            [
                "We can observe that adding BatchNorm is beneficial for either DiffCSE or SimCSE to get better performance on STS-B and transfer tasks."
            ]
        ],
        [
            [
                "<edit>",
                1,
                "<del>",
                "Different",
                "</del>",
                "Size",
                "of",
                "the",
                "Generator",
                " </edit>"
            ],
            [
                "In our DiffCSE model, the generator can be in different model size from BERT large, BERT base (Devlin et al., 2019), DistilBERT base (Sanh et al., 2019),BERT medium, BERT small, BERT mini, BERT tiny (Turc et al., 2019)."
            ],
            [
                "Their exact sizes are shown in BERT's performance due to knowledge distillation."
            ]
        ],
        [
            [
                "We show our results in Table 6, we can see the performance of transfer tasks does not change much with different generators."
            ],
            [
                "However, the score of STS-B decreases as we switch from BERTmedium to BERT-tiny."
            ],
            [
                "This finding is not the same as ELECTRA, which works best with generators 1/4-1/2 the size of the discriminator."
            ],
            [
                "Because our discriminator is conditional on sentence vectors, it will be easier for the discriminator to perform the RTD task."
            ],
            [
                "As a result, using stronger generators (BERT base , DistilBERT base ) to increase the difficulty of RTD would help the discriminator learn better."
            ],
            [
                "However, when using a large model like BERT large , it may be a too-challenging task for the discriminator."
            ],
            [
                "In our experiment, using DistilBERT base , which has the ability close to but slightly worse than BERT base , gives us the best performance."
            ]
        ],
        [
            [
                "Masking Ratio In our conditional ELECTRA task, we can mask the original sentence in different ratios for the generator to produce MLM-based augmentations."
            ],
            [
                "A higher masking ratio will make more perturbations to the sentence."
            ],
            [
                "Our empirical result in Table 7 shows that the difference between difference masking ratios is small (in 15%-40% ), and a masking ratio of around 30% can give us the best performance."
            ]
        ],
        [
            [
                "Coefficient \u03bb In Section 3, we use the \u03bb coefficient to weight the ELECTRA loss and then add it with contrastive loss."
            ],
            [
                "Because the contrastive learning objective is a relatively easier task, the scale of contrastive loss will be 100 to 1000 smaller than ELECTRA loss."
            ],
            [
                "As a result, we need a smaller \u03bb to balance these two loss terms."
            ],
            [
                "In the Table 8 we show the STS-B result under different \u03bb values."
            ]
        ],
        [
            [
                "Note that when \u03bb goes to zero, the model becomes a SimCSE model."
            ],
            [
                "We find that using \u03bb = 0.005 can give us the best performance."
            ]
        ],
        [
            [
                "6 Analysis"
            ]
        ],
        [
            [
                "Qualitative Study"
            ]
        ],
        [
            [
                "A very common application for sentence embeddings is the retrieval task."
            ],
            [
                "Here we show some retrieval examples to qualitatively explain why Dif-fCSE can perform better than SimCSE."
            ],
            [
                "In this study, we use the 2758 sentences from STS-B testing set as the corpus, and then use sentence query to retrieve the nearest neighbors in the sentence embedding space by computing cosine similarities."
            ]
        ],
        [
            [
                "We show the retrieved top-3 examples in Table 9."
            ],
            [
                "The first query sentence is \"you can do it, too.\"."
            ],
            [
                "The SimCSE model retrieves a very similar sentence but has a slightly different meaning (\"you can use it, too.\")"
            ],
            [
                "as the rank-1 answer."
            ],
            [
                "In contrast, DiffCSE can distinguish the tiny difference, so it retrieves the ground truth answer as the rank-1 answer."
            ],
            [
                "The second query sentence is \"this is not a problem\"."
            ],
            [
                "SimCSE retrieves a sentence with opposite meaning but very similar wording, while DiffCSE can retrieve the correct answer with less similar wording."
            ],
            [
                "We also provide a third example where both SimCSE and DiffCSE fail to retrieve the correct answer for a query sentence using double negation."
            ]
        ],
        [
            [
                "Retrieval Task"
            ]
        ],
        [
            [
                "Besides the qualitative study, we also show the quantitative result of the retrieval task."
            ],
            [
                "Here we also use all the 2758 sentences in the testing set of STS-B as the corpus."
            ],
            [
                "There are 97 positive pairs in this corpus (with 5 out of 5 semantic similarity scores from human annotation)."
            ],
            [
                "For each positive pair, we use one sentence to retrieve the other one, and see whether the other sentence is in the top-1/5/10 ranking."
            ],
            [
                "The recall@1/5/10 of the retrieval task are shown in 3) can you do it?"
            ]
        ],
        [
            [
                "Query: this is not a problem."
            ]
        ],
        [
            [
                "1) this is a big problem."
            ],
            [
                "1) i don 't see why this could be a problem."
            ],
            [
                "2) you have a problem."
            ]
        ],
        [
            [
                "2) i don 't see why that should be a problem."
            ],
            [
                "3) i don 't see why that should be a problem."
            ]
        ],
        [
            [
                "3) this is a big problem."
            ]
        ],
        [
            [
                "Query: i think that is not a bad idea."
            ],
            [
                "1) i do not think it's a good idea."
            ],
            [
                "1) i do not think it's a good idea ."
            ],
            [
                "2) it's not a good idea ."
            ]
        ],
        [
            [
                "2) it is not a good idea."
            ],
            [
                "3) it is not a good idea ."
            ]
        ],
        [
            [
                "3) but it is not a good idea."
            ],
            [
                "recall@1/5/10, showing the effectiveness of using DiffCSE for the retrieval task."
            ]
        ],
        [
            [
                "Distribution of Sentence Embeddings"
            ]
        ],
        [
            [
                "To look into the representation space of DiffCSE, we plot the cosine similarity distribution of sentence pairs from STS-B test set for both SimCSE and DiffCSE in Figure 2."
            ],
            [
                "We observe that both SimCSE and DiffCSE can assign cosine similarities consistent with human ratings."
            ],
            [
                "However, we also observe that under the same human rating, DiffCSE assigns slightly higher cosine similarities compared with SimCSE."
            ],
            [
                "This phenomenon may be caused by the fact that ELECTRA and other Transformer-based pretrained LMs have the problem of squeezing the representation space, as mentioned by Meng et al."
            ],
            [
                "(2021)."
            ],
            [
                "As we use the sentence embeddings as the input of ELECTRA to perform conditional ELECTRA training, the sentence embedding will be inevitably squeezed to fit the input distribution of ELECTRA."
            ],
            [
                "We follow prior studies (Wang and Isola, 2020;Gao et al., 2021)"
            ]
        ],
        [
            [
                "Conclusion"
            ]
        ],
        [
            [
                "In this paper, we present DiffCSE, a new unsupervised sentence embedding framework that is aware of, but not invariant to, MLM-based word replacement."
            ],
            [
                "Empirical results on semantic textual similarity tasks and transfer tasks both show the effectiveness of DiffCSE compared to current stateof-the-art sentence embedding methods."
            ],
            [
                "We also conduct extensive ablation studies to demonstrate the different modeling choices in DiffCSE."
            ],
            [
                "Qualitative study and the retrieval results also show that DiffCSE can produce a better embedding space for sentence retrieval."
            ],
            [
                "One limitation of our work is that we do not explore the supervised setting that uses human-labeled NLI datasets to further boost the performance."
            ],
            [
                "We leave this topic for future work."
            ],
            [
                "We believe that our work can provide researchers in the NLP community a new way to utilize augmentations for natural language and thus produce better sentence embeddings."
            ]
        ],
        [
            [
                "<edit>",
                3,
                "<add>",
                "Implementation Details",
                "</add>",
                "</edit>"
            ],
            [
                "<edit>",
                3,
                "<add>",
                "We use a single NVIDIA 2080Ti GPU for each experiment.",
                "</add>",
                "</edit>"
                
            ],
            [
                "<edit>",
                3,
                "<add>",
                "The averaged running time for DiffCSE is 3-6 hours.",
                "</add>",
                "</edit>"
            ],
            [
                "<edit>",
                3,
                "<add>",
                "We use gridsearch of batch size \u2208 {64, 128} learning rate \u2208 {2e-6, 3e-6, 5e-6, 7e-6, 1e-5} and masking ratio \u2208 {0.15, 0.20, 0.30, 0.40} and \u03bb \u2208 {0.1, 0.05, 0.01, 0.005, 0.001}.",
                "</add>",
                "</edit>"
            ],
            [
                "<edit>",
                3,
                "<add>",
                "The temperature \u03c4 in SimCSE is set to 0.05 for all the experiments.",
                "</add>",
                "</edit>"
            ],
            [
                "<edit>",
                3,
                "<add>",
                "During the training process, we save the checkpoint with the highest score on the STS-B development set.",
                "</add>",
                "</edit>"
            ],
            [
                "<edit>",
                3,
                "<add>",
                "And then we use STS-B development set to find the best hyperparameters (listed in Table 12) for STS task; we use the averaged score of the development sets of 7 transfer tasks to find the best hyperparameters (listed in Table 13) for transfer tasks.",
                "</add>",
                "</edit>"
            ]
        ],
        [
            [
                "In Section 5, we try to use different augmentations (e.g."
            ],
            [
                "insertion, deletion, replacement) for learning equivariance."
            ],
            [
                "In Table 15 we provide the results of using these augmentations as additional positive or negative examples along with the SimCSE training paradigm."
            ],
            [
                "We can observe that using these augmentations as additional positives only decreases the performance."
            ],
            [
                "The only method that can improve the performance a little bit is to use MLM 15% replaced examples as additional negative examples."
            ],
            [
                "Overall, none of these results can perform better than our proposed method, e.g."
            ],
            [
                "using these augmentations to learn equivariance."
            ]
        ],
        [
            [
                "Wang and Isola (2020) propose to use two properties, alignment and uniformity, to measure the quality of representations."
            ],
            [
                "Given a distribution of positive pairs p pos and the distribution of the whole dataset p data , alignment computes the expected distance between normalized embeddings of the paired sentences:"
            ]
        ],
        [
            [
                "Uniformity measures how well the embeddings are uniformly distributed in the representation space:"
            ]
        ],
        [
            [
                "\u223c p data e \u22122\u2225f (x)\u2212f (y)\u2225 2 ."
            ]
        ],
        [
            [
                "The smaller the values of uniformity and alignment, the better the quality of the representation space is indicated."
            ]
        ],
        [
            [
                "<edit>",
                1,
                "We",
                "build",
                "our",
                "model",
                "using",
                "the",
                "PyTorch",
                "implementation",
                "of",
                "SimCSE",
                "<del>",
                "6",
                "</del>",
                "<add>",
                "7",
                "</add>",
                "Gao",
                "et",
                "al",
                "<punc>",
                ".",
                "</punc>",
                " </edit>"
            ],
            [
                "(2021), which is based on the HuggingFace's Transformers package."
            ],
            [
                "<edit>",
                1,
                "<del>",
                "7",
                "</del>",
                "<add>",
                "8",
                "</add>",
                "We",
                "also",
                "upload",
                "our",
                "code",
                "<del>",
                "8",
                "</del>",
                "<add>",
                "9",
                "</add>",
                "and",
                "pretrained",
                "models",
                "(",
                "links",
                "in",
                "README.md",
                ")",
                "<punc>",
                ".",
                "</punc>",
                " </edit>"
            ],
            [
                "Please follow the instructions in README.md to reproduce the results."
            ]
        ],
        [
            [
                "<edit>",
                1,
                "On",
                "the",
                "risk",
                "side",
                "<punc>",
                ",",
                "</punc>",
                "insofar",
                "as",
                "our",
                "method",
                "utilizes",
                "pretrained",
                "language",
                "models",
                "<punc>",
                ",",
                "</punc>",
                "it",
                "may",
                "inherit",
                "and",
                "propagate",
                "some",
                "of",
                "the",
                "<del>",
                "harmful",
                "</del>",
                "biases",
                "present",
                "in",
                "such",
                "models",
                "<punc>",
                ".",
                "</punc>",
                " </edit>"
            ],
            [
                "Besides that, we do not see any other potential risks in our paper."
            ]
        ]
    ]
}